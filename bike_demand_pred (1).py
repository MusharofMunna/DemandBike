# -*- coding: utf-8 -*-
"""Bike Demand Pred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/109jEc7O_nTHWZpp1lslLsZ0Qh6bmRcFR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import requests
import json

# Handle dates and times in our bike data
from datetime import datetime, timedelta

# Track processing time - might need this for large datasets
import time

# Configure visualizations
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
pd.options.mode.chained_assignment = None  # Suppress warnings

# Fetch station data
print("Fetching station data from TfL API...")

base_url = "https://api.tfl.gov.uk/BikePoint"
try:
    # Get all bike stations
    response = requests.get(base_url)
    if response.status_code == 200:
        stations = response.json()
        print(f"Successfully fetched data for {len(stations)} bike stations")
    else:
        print(f"Error fetching data: Status code {response.status_code}")
except Exception as e:
    print(f"Something went wrong: {str(e)}")

# Explore API data
print("\nFields in the dataset:")
for key in stations[0].keys():
    print(f": {key}")

# Check values from a sample station
if stations:
    sample_station = stations[0]
    print("Sample station data:")
    for key, value in sample_station.items():
        print(f"{key}: {value}")

# Explore Additional Properties
print("\nadditionalProperties:")
for prop in sample_station['additionalProperties']:
    print(f"Property: {prop['key']} = {prop['value']}")

"""To-Do:
1. Selected the target attributes needed for the project
2. Create a copy with the target attributes
2. Running further analysis on the copy of the dataset
"""

# Create a copy of the dataset for exploration
stations_list = []
for station in stations:
    # Store basic info, .get() for safety
    station_info = {
        'station_id': station.get('id', 'unknown').replace('BikePoints_', ''),
        'name': station.get('commonName', 'unnamed'),
        'latitude': station.get('lat', None),
        'longitude': station.get('lon', None)
    }
    stations_list.append(station_info)

df_stations = pd.DataFrame(stations_list)

# Attribute Characteristics
for column in df_stations.columns:
    print(f"\nAttribute: {column}")
    print(f"Type: {df_stations[column].dtype}")
    print(f"% Missing Values: {df_stations[column].isnull().mean() * 100:.2f}%")
    if df_stations[column].dtype in ['int64', 'float64']:
        print(f"Min: {df_stations[column].min()}, Max: {df_stations[column].max()}")
        print(f"Mean: {df_stations[column].mean()}, Std: {df_stations[column].std()}")
    elif df_stations[column].dtype == 'object':
        print(f"Unique Values: {df_stations[column].nunique()}" )

# station_id is object data type, convert to int for future use
df_stations['station_id'] = pd.to_numeric(df_stations['station_id'], errors='coerce')
df_stations.dropna(subset=['station_id'], inplace=True)
df_stations['station_id'] = df_stations['station_id'].astype(int)

#check station_id
print(f"station_id:  {df_stations['station_id'].dtype}")

# Remove spaces and convert names to lowercase
print("\nPerforming consistency check for station names...")
df_stations['name'] = df_stations['name'].str.strip().str.lower()

# Detect outliers
lat_bounds = (51.3, 51.6)  # Expected range for London
lon_bounds = (-0.3, 0.1)

# Flag outliers
df_stations['latitude_outlier'] = ~df_stations['latitude'].between(lat_bounds[0], lat_bounds[1])
df_stations['longitude_outlier'] = ~df_stations['longitude'].between(lon_bounds[0], lon_bounds[1])

# Reporting outliers
lat_outliers = df_stations[df_stations['latitude_outlier']]
lon_outliers = df_stations[df_stations['longitude_outlier']]
print(f"Total Latitude Outliers: {lat_outliers.shape[0]}")
print(f"Total Longitude Outliers: {lon_outliers.shape[0]}")

# Remove outlier flags
df_stations.drop(columns=['latitude_outlier', 'longitude_outlier'], inplace=True)

"""### Adding Historical Weather Data"""

# Define API details
weather_api_url = "https://api.worldweatheronline.com/premium/v1/past-weather.ashx"
api_key = "523cd7e829d74550a3013513243011"

# Set location and date range
location = "London,UK"
start_date = "2019-01-01"
end_date = "2019-12-31"

# Fetch weather data
params = {
    'q': location,
    'date': start_date,
    'enddate': end_date,
    'tp': 3,  # 3-hour interval
    'format': 'json',
    'key': api_key
}

try:
    response = requests.get(weather_api_url, params=params)
    if response.status_code == 200:
        weather_data = response.json()
        print("Successfully fetched historical weather data")
        # Save weather data for processing
        with open("weather_data.json", "w") as outfile:
            json.dump(weather_data, outfile)
    else:
        print(f"Error fetching weather data: Status code {response.status_code}")
except Exception as e:
    print(f"Error: {str(e)}")

# Parsing weather data into a DataFrame
def parse_weather_data(weather_data):
    # Extract relevant data from JSON response
    all_weather_data = []
    for day_data in weather_data['data']['weather']:
        date = day_data['date']
        for hourly_data in day_data['hourly']:
            all_weather_data.append({
                'date': date,
                'time': int(hourly_data['time']),
                'temperature_c': float(hourly_data['tempC']),
                'precipitation_mm': float(hourly_data['precipMM']),
                'humidity': int(hourly_data['humidity']),
                'wind_speed_kph': int(hourly_data['windspeedKmph']),
                'weather_desc': hourly_data['weatherDesc'][0]['value']
            })
    return pd.DataFrame(all_weather_data)

# Load and process weather data
try:
    with open("weather_data.json", "r") as infile:
        raw_weather_data = json.load(infile)
        df_weather = parse_weather_data(raw_weather_data)
        print("Weather DataFrame created successfully!")
        print(df_weather.head())
except FileNotFoundError:
    print("Weather data file not found. Ensure data fetching is successful.")

# Basic statistics and data quality check
print("\nWeather Data Overview:")
print(df_weather.info())
print("\nBasic Statistics:")
print(df_weather.describe())

# Check for missing values
print("\nMissing Values:")
print(df_weather.isnull().sum())

# Temperature distribution
plt.figure(figsize=(12, 6))
sns.histplot(data=df_weather, x='temperature_c', bins=30)
plt.title('Distribution of Temperature in London (2019)')
plt.xlabel('Temperature (Â°C)')
plt.ylabel('Count')
plt.show()

# Precipitation patterns
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_weather, x='weather_desc', y='precipitation_mm')
plt.xticks(rotation=45)
plt.title('Precipitation by Weather Description')
plt.tight_layout()
plt.show()

# Monthly weather patterns
df_weather['month'] = pd.to_datetime(df_weather['date']).dt.month
monthly_weather = df_weather.groupby('month').agg({
    'temperature_c': 'mean',
    'precipitation_mm': 'mean',
    'humidity': 'mean',
    'wind_speed_kph': 'mean'
}).round(2)

print("\nMonthly Weather Patterns:")
print(monthly_weather)

# Weather description distribution
plt.figure(figsize=(12, 6))
df_weather['weather_desc'].value_counts().plot(kind='bar')
plt.title('Distribution of Weather Conditions')
plt.xlabel('Weather Description')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df_weather.select_dtypes(include=[np.number]).corr(),
            annot=True,
            cmap='coolwarm',
            center=0)
plt.title('Weather Features Correlation')
plt.tight_layout()
plt.show()

from google.colab import drive
drive.mount('/content/drive')

#access the downloaded dataset files in google drive
data_folder = '/content/drive/MyDrive/Colab Notebooks/ML Project/Tfl journey datasets 2019'

all_data = []

# Loop through each file in the folder
for file in os.listdir(data_folder):
    if file.endswith(".csv"):
        file_path = os.path.join(data_folder, file)
        # Load CSV as dataframe
        df = pd.read_csv(file_path)
        # Append it to list
        all_data.append(df)

# Combine all DataFrames into one
df_historical = pd.concat(all_data, ignore_index=True)

# Preview the combined DataFrame
print(f"Total rows in combined data: {df_historical.shape[0]}")
print(df_historical.head())

# Explore dataset
print("\n2019 Data Overview:")
print("Shape:", df_historical.shape)
print("\nColumns:", df_historical.columns.tolist())
print("\nData Types:")
print(df_historical.dtypes)

# simplifying column names
df_historical.columns = df_historical.columns.str.lower().str.replace(' ', '_')
print("\nUpdated Column Names:")
print(df_historical.columns)

# Basic statistics
print("\nBasic Statistics:")
print(df_historical.describe())

# Check for missing values
print("\nMissing Values:")
print(df_historical.isnull().sum())

# date range
if 'start_date' in df_historical.columns:
    print("\nDate Range:")
    print("Start:", df_historical['start_date'].min())
    print("End:", df_historical['start_date'].max())

"""**Note**
While checking date range above, found the following issue : minimum start date starts from 01/02/2019. It doesn't include january (I have january datasets included)

**To do**
Convert start and end dates into datetime format
"""

# Convert to datetime (fix)
df_historical['start_date'] = pd.to_datetime(df_historical['start_date'], format='%d/%m/%Y %H:%M')
df_historical['end_date'] = pd.to_datetime(df_historical['end_date'], format='%d/%m/%Y %H:%M')

# ReCheck date range to ensure issue fix
if 'start_date' in df_historical.columns:
    print("\nDate Range:")
    print("Start:", df_historical['start_date'].min())
    print("End:", df_historical['start_date'].max())

print("\nData Types after datetime conversion:")
print(df_historical.dtypes)

"""**Feature Engineering**"""

# Create additional time-based features
df_historical['hour'] = df_historical['start_date'].dt.hour
df_historical['day'] = df_historical['start_date'].dt.day
df_historical['month'] = df_historical['start_date'].dt.month
df_historical['day_of_week'] = df_historical['start_date'].dt.dayofweek
df_historical['is_weekend'] = df_historical['day_of_week'].isin([5,6]).astype(int)

# Analyze patterns
print("\nBasic Journey Statistics:")
print(f"Total Journeys: {len(df_historical):,}")
print(f"Unique Bikes Used: {df_historical['bike_id'].nunique():,}")
print(f"Unique Start Stations: {df_historical['startstation_id'].nunique():,}")
print(f"Average Journey Duration: {df_historical['duration'].mean():.1f} seconds")

#Station popularity analysis
print("\nTop 5 Busiest Start Stations:")
print(df_historical['startstation_name'].value_counts().head())

print("\nTop 5 Busiest End Stations:")
print(df_historical['endstation_name'].value_counts().head())

# Time-based patterns
print("\nJourneys by Day of Week:")
day_stats = df_historical.groupby('day_of_week').size()
print(day_stats)

print("\nJourneys by Hour:")
hour_stats = df_historical.groupby('hour').size()
print(hour_stats)

"""### Visual Analysis"""

# Hourly pattern
plt.figure(figsize=(12, 6))
hourly_rides = df_historical.groupby('hour').size()
hourly_rides.plot(kind='bar')
plt.title('Rides by Hour of Day')
plt.xlabel('Hour')
plt.ylabel('Number of Rides')
plt.show()

# Daily pattern

plt.figure(figsize=(12, 6))
daily_rides = df_historical.groupby('day_of_week').size()
daily_rides.plot(kind='bar')
plt.title('Rides by Day of Week')
plt.xlabel('Day (0=Monday, 6=Sunday)')
plt.ylabel('Number of Rides')
plt.show()

# Monthly pattern

plt.figure(figsize=(12, 6))
monthly_rides = df_historical.groupby('month').size()
monthly_rides.plot(kind='bar')
plt.title('Rides by Month')
plt.xlabel('Month')
plt.ylabel('Number of Rides')
plt.show()

# Journey Duration Distribution

plt.figure(figsize=(12, 6))
df_historical['duration_minutes'] = df_historical['duration'] / 60
sns.histplot(data=df_historical[df_historical['duration_minutes'] <= 120],
             x='duration_minutes',
             bins=50)
plt.title('Journey Duration Distribution')
plt.xlabel('Duration (minutes)')
plt.ylabel('Count')
plt.show()

"""### **Note**
Temporal Patterns in London Bike-Sharing (2019)

**Hourly Usage Patterns**

**Peak Hours:**

Morning Peak: 8AM (1.1M rides)

Evening Peak: 17:00/5PM (~1.1M rides)

**Off-Peak:**

Lowest usage: 2AM-5AM

Mid-day plateau: 10AM-4PM

Pattern indicates:

Strong commuter usage

Clear business hour influence

Minimal night-time activity

**Daily Usage Patterns**

Weekday vs Weekend:

Weekdays (Mon-Fri): Higher consistent usage

Weekends: Significant drop in usage

**Specific Days:**

Highest: Wednesday (1.65M rides)

Lowest: Sunday (1.15M rides)

**Pattern suggests:**

Primary use for commuting

Business day dependency

Weekend leisure usage is lower

**Monthly/Seasonal Patterns**

**Peak Season:**

Highest: July (1.1M rides)

Strong summer usage

**Low Season:**

Winter months (Dec-Feb)

Lowest in December

**Trend:**

Gradual increase SpringâSummer

Gradual decrease AutumnâWinter

Shows clear seasonal influence

**Journey Duration Analysis**

**Typical Journey:**

Most common: 5-20 minutes

Peak duration: 10 minutes

**Distribution:**

Right-skewed distribution

Sharp decline after 20 minutes

Very few rides >60 minutes


"""

# Additional time features
df_historical['hour_sin'] = np.sin(2 * np.pi * df_historical['hour']/24)  # Cyclical encoding
df_historical['hour_cos'] = np.cos(2 * np.pi * df_historical['hour']/24)
df_historical['peak_hour'] = df_historical['hour'].apply(lambda x: 1 if x in [8,9,17,18] else 0)  # Rush hours
df_historical['part_of_day'] = pd.cut(df_historical['hour'],
                                    bins=[-1,6,12,18,23],
                                    labels=['Night','Morning','Afternoon','Evening'])

# Additional journey metrics
df_historical['same_station'] = (df_historical['startstation_id'] == df_historical['endstation_id']).astype(int)
df_historical['journey_count_per_day'] = df_historical.groupby(df_historical['start_date'].dt.date)['rental_id'].transform('count')
df_historical['avg_duration_per_day'] = df_historical.groupby(df_historical['start_date'].dt.date)['duration'].transform('mean')

# Station demand metrics
station_stats = df_historical.groupby('startstation_id').agg({
    'rental_id': 'count',
    'duration': ['mean', 'std'],
    'same_station': 'mean'
}).reset_index()

station_stats.columns = ['startstation_id', 'total_trips', 'avg_duration', 'duration_std', 'return_rate']

# Just copy the needed columns, no conversion needed
station_locations = df_stations[['station_id', 'latitude', 'longitude']].copy()

# Do both merges cleanly
df_historical = (df_historical
                .merge(station_stats, on='startstation_id', how='left')
                .merge(station_locations,
                      left_on='startstation_id',
                      right_on='station_id',
                      how='left')
                .drop('station_id', axis=1))  # drop the redundant column

# Time-based aggregations
hourly_stats = df_historical.groupby(['start_date', 'hour']).agg({
    'rental_id': 'count',
    'duration': 'mean',
    'startstation_id': 'nunique'
}).reset_index()

# Calculate moving averages
hourly_stats['MA_24h'] = hourly_stats['rental_id'].rolling(24).mean()
hourly_stats['MA_7d'] = hourly_stats['rental_id'].rolling(24*7).mean()

# Visualize trends
plt.figure(figsize=(15, 6))
plt.plot(hourly_stats['start_date'], hourly_stats['rental_id'], alpha=0.5, label='Actual')
plt.plot(hourly_stats['start_date'], hourly_stats['MA_24h'], label='24-hour MA')
plt.plot(hourly_stats['start_date'], hourly_stats['MA_7d'], label='7-day MA')
plt.title('Rental Trends Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Rentals')
plt.legend()
plt.show()

# Create geographical visualization with improved aesthetics
plt.figure(figsize=(12, 8))
plt.scatter(df_historical['longitude'],
           df_historical['latitude'],
           c=df_historical['total_trips'],
           cmap='YlOrRd',
           alpha=0.6,
           s=100)  # increased point size for better visibility

plt.colorbar(label='Number of Trips')
plt.title('London Bike Station Usage Heat Map\nSize and Color Intensity Indicate Number of Trips', pad=20)
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Add grid for better geographical reference
plt.grid(True, alpha=0.3)

# Adjust layout
plt.tight_layout()
plt.show()

